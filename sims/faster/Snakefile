# pipeline to infer dispersal rates and locate genetic ancestors with spacetrees (Osmond & Coop 2024)

datadir = '../data/' #relative path to data directory
base = datadir + 'sim_{L}L_{RBP}RBP_{LAMBDA}LAMBDA_{K}K_{W}W_{SIGMAcomp}SIGMAcomp_{SIGMAmate}SIGMAmate_{SIGMAdisp}SIGMAdisp_{selfing}selfing_{MAXT}MAXT_{nrep}nrep_{Ne}Ne_{U}U_{d}d_{k}k_{G}G'
locations = base + '_locs.txt'
coal = base + '_{numiter}numiter_{threshold}threshold.coal'
newick = base + '_{numiter}numiter_{threshold}threshold_{treeskip}treeskip_{M}M.newick'

# wildcards determining input files
Ls = [int(1e8)] #number of basepairs
RBPs = [1e-8] #per base pair recombination rate
LAMBDAs = [2.0] #mean number of offspring per parent when no competition
Ks = [1.0] #effect of competition on birth (higher values less effect)
Ws = [100] #width of habitat square
SIGMAcomps = [1.0] #SD of competition kernel
SIGMAmates = [0.5] #SD of mate choice kernel
SIGMAdisps = [0.25,0.5,0.75] #,0.5,1.0,2.0] #SD of dispersal kernal
selfings = [0] #fraction of offspring selfed
MAXTs = [40000] #number of gens to run sim for (aim for 4*W**2*K, the expected time to an MRCA in a panmictic population) 
NREPS = 10 #number of reps
nreps = range(NREPS)
Nes = [10000] #Ne for recapitation, make roughly W**2 * K
Us = [1e-8] #per base pair mutation rate
ds = [100] #[10, 100] #max distance from center of habitat to sample
ks = [25,50,100] #number of diploid individuals to sample
#ks = [50] #number of diploid individuals to sample
Gs = [2] #number of genomes per individual to sample (number of sample nodes is k*G)
numiters = [5] #number of mcmc iterations to do when inferring coalescence rates (5 is default)
thresholds = [0.5] #fraction of trees to drop when inferring coalescence relates (0.5 is default)
treeskips = [100] #use every TREESKIPth tree for inference
Ms = [1000] #number of samples of branch lengths at each tree for importance sampling

# wildcards that get used below
Ts = [100,1000,None] #[100] #,None] #when to chop subtrees
Ts = [None]
ms = [1,10,100] #number of importance samples to actually use
ms = [100]
ns = [1,10,100]
ns = [100] #max number of loci to use
ancestor_times = range(100,1001,100) #generations (from present) to locate ancestors in

# ---------------- extract times from trees -----------------------------

# now we will extract the information we need from the trees, the shared times between each pair of lineages and the coalescence times

shared_times = newick.replace('.newick','_{m}m_{n}n.stss') #allowing one to use just the first m importance samples and first n loci
coal_times = newick.replace('.newick','_{m}m_{n}n.ctss')

rule extract_times:
  input:
    newick=newick 
  output:
    stss=shared_times,
    ctss=coal_times
  group: "extract_times"
  threads: 1
  resources:
    runtime=15
  run:
    # prevent numpy from using more than {threads} threads (useful for parallizing on my server)
    import os
    os.environ["OMP_NUM_THREADS"] = str(threads)
    os.environ["GOTO_NUM_THREADS"] = str(threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(threads)
    os.environ["MKL_NUM_THREADS"] = str(threads)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(threads)

    # import tools
    import numpy as np
    from tsconvert import from_newick
    from utils import get_shared_times
    from tqdm import tqdm

    t = tqdm(total=int(wildcards.n))

    # open file of trees to read from
    with open(input.newick, mode='r') as f:
      
      # open files to append to
      with open(output.stss, 'a') as stss:
        with open(output.ctss, 'a') as ctss:

          i = 0
          j = 0
          for line in f: #for each tree sampled

            if i<int(wildcards.m):
    
              # import tree
              string = line.split()[4] #extract newick string only (Relate adds some info beforehand)
              ts = from_newick(string, min_edge_length=1e-6) #convert to tskit "tree sequence" (only one tree)
              tree = ts.first() #the only tree
    
              # get shared times
              samples = [int(ts.node(node).metadata['name']) for node in ts.samples()] #get index of each sample in list we gave to relate
              sample_order = np.argsort(samples) #get indices to put in ascending order
              ordered_samples = [ts.samples()[i] for i in sample_order] #order samples as in relate
              sts = get_shared_times(tree, ordered_samples) #get shared times between all pairs of samples, with rows and columns ordered as in relate
              stss.write(",".join([str(i) for i in sts]) + '\n') #append as new line
   
              # get coalescence times 
              cts = sorted([tree.time(i) for i in tree.nodes() if not tree.is_sample(i)]) #coalescence times, in ascending order
              ctss.write(",".join([str(i) for i in cts]) + '\n') #append as new line

              # next tree
              i += 1

            elif i<int(wildcards.M):
              i += 1

            elif i==int(wildcards.M):
              i = 0
              j += 1
              t.update()
              if j == int(wildcards.n):
                break

          t.close()    
# ---------------- process times -----------------------------

# now we process the times, potentially cutting off the tree (to ignore distant past) and getting the exact quantities we need for inference

processed_times = shared_times.replace('.stss','_{T}T.{end}')
ends = ['stss_logdet','stss_inv','btss','lpcs']

rule process_times:
  input:
    stss = shared_times,
    ctss = coal_times,
    coal = coal
  output:
    expand(processed_times, end=ends, allow_missing=True)
  group: "process_times"
  threads: 1 
  resources:
    runtime=15
  run:
    # prevent numpy from using more than {threads} threads (useful for parallizing on my server)
    import os
    os.environ["OMP_NUM_THREADS"] = str(threads)
    os.environ["GOTO_NUM_THREADS"] = str(threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(threads)
    os.environ["MKL_NUM_THREADS"] = str(threads)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(threads)

    # load tools
    import numpy as np
    from utils import chop_shared_times, center_shared_times, log_coal_density
    from tqdm import tqdm

    # determine time cutoff
    T = wildcards.T #get time cutoff
    T = None if T=='None' else float(T) #format correctly

    # effective population size
    epochs = np.genfromtxt(input.coal, skip_header=1, skip_footer=1) #time at which each epoch starts (and the final one ends)
    Nes = 0.5/np.genfromtxt(input.coal, skip_header=2)[2:] #effective population size during each epoch

    # open file of shared times to read from
    with open(input.stss, 'r') as stss:
      with open(input.ctss, 'r') as ctss:

        # open files to write to
        with open(output[0], 'a') as stss_logdet:
          with open(output[1], 'a') as stss_inverted:
            with open(output[2], 'a') as btss:
              with open(output[3], 'a') as lpcs:
            
                # loop over trees
                for sts,cts in zip(stss,ctss):
            
                  # load shared time matrix in vector form
                  sts = np.fromstring(sts, dtype=float, sep=',') #convert from string to numpy array
    
                  # chop
                  sts = chop_shared_times(sts, T=T) #chop shared times to ignore history beyond T
                  
                  # convert to matrix form
                  k = int((np.sqrt(1+8*(len(sts)-1))+1)/2) #get number of samples (from len(sts) = k(k+1)/2 - k + 1)
                  sts_mat = np.zeros((k,k)) #initialize matrix
                  sts_mat[np.triu_indices(k, k=1)] = sts[1:] #fill in upper triangle
                  sts_mat = sts_mat + sts_mat.T + np.diag([sts[0]]*k) #add lower triangle and diagonal
                  sts = sts_mat
    
                  # center
                  sts = center_shared_times(sts) 
            
                  # determinant
                  sts_logdet = np.linalg.slogdet(sts)[1] #magnitude of log determinant (ignore sign)
                  stss_logdet.write(str(sts_logdet) + '\n') #append as new line 
            
                  # inverse
                  sts = np.linalg.inv(sts) #inverse
                  sts = sts[np.triu_indices(k-1, k=0)] #convert to list
                  stss_inverted.write(",".join([str(i) for i in sts]) + '\n') #append as new line

                  # branching times
                  cts = np.fromstring(cts, dtype=float, sep=',') 
                  Tmax = cts[-1] #time to most recent common ancestor
                  if T is not None and T < Tmax:
                      Tmax = T #farthest time to go back to
                  bts = Tmax - np.flip(cts) #branching times, in ascending order
                  bts = bts[bts>0] #remove branching times at or before T
                  bts = np.append(bts, Tmax) #append total time as last item      
                  btss.write(",".join([str(i) for i in bts]) + '\n') #append as new line
                  
                  # probability of coalescence times under neutral coalescent
                  lpc = log_coal_density(times=cts, Nes=Nes, epochs=epochs, T=Tmax) #log probability density of coalescence times
                  lpcs.write(str(lpc) + '\n') #append as new line 

# ----------- estimate dispersal ------------------------

# and now we bring in our processed times across chromosomes and loci to estimate a dispersal rate

dispersal_rate = processed_times.replace('{end}','sigma')

rule dispersal_rate:
  input:
    stss_logdet = processed_times.replace('{end}','stss_logdet'),
    stss_inv = processed_times.replace('{end}','stss_inv'),
    btss = processed_times.replace('{end}','btss'),
    lpcs = processed_times.replace('{end}','lpcs'),
    locations = locations
  output:
    sigma = dispersal_rate
  group: "dispersal_rate"
  threads: 1
  resources:
    runtime=15
  run:
    # prevent numpy from using more than {threads} threads (useful for parallizing on my server)
    import os
    os.environ["OMP_NUM_THREADS"] = str(threads)
    os.environ["GOTO_NUM_THREADS"] = str(threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(threads)
    os.environ["MKL_NUM_THREADS"] = str(threads)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(threads)

    # load tools
    import numpy as np
    from tqdm import tqdm
    from spacetrees import estimate_dispersal

    # load input data
    stss_logdet = np.loadtxt(input.stss_logdet) #log determinants of chopped and centered shared times matrices
    M = int(wildcards.m) #number of importance samples
    LM = len(stss_logdet)
    L = int(LM/M) #number of loci
    stss_logdet = stss_logdet.reshape(L,M,1) #group by locus for dispersal function
    print('building inverted shared times matrices')
    k = 2*int(wildcards.k) - 1 #number of samples minus 1 (mean centered)
    with open(input.stss_inv, 'r') as f: #vectorized chopped, centered, and inverted shared time matrices
      stss_inv = [] #list of inverses in matrix form
      for line in tqdm(f, total=LM):
        sts_inv = np.fromstring(line, dtype=float, sep=',')
        mat = np.zeros((k,k))
        mat[np.triu_indices(k, k=0)] = sts_inv #convert to numpy matrix
        mat = mat + mat.T - np.diag(np.diag(mat))      
        stss_inv.append(mat)
    stss_inv = [stss_inv[i:i+M] for i in range(0,LM,M)] #group by locus for dispersal function
    btss = []
    print('loading branching times')
    with open(input.btss, 'r') as f: #branching times
      for line in tqdm(f, total=LM):
        btss.append(np.fromstring(line, dtype=float, sep=','))
    btss = [btss[i:i+M] for i in range(0,LM,M)] #group by locus for dispersal function
    lpcs = np.loadtxt(input.lpcs) #log probability of coalescence times
    lpcs = lpcs.reshape(L,M) #group by locus for dispersal function
    locations = np.loadtxt(input.locations) #location of each sample

    # estimate dispersal rate
    def callbackF(x):
      '''print updates during numerical search'''
      print('{0: 3.6f}   {1: 3.6f}   {2: 3.6f}   {3: 3.6f}'.format(x[0], x[1], x[2], x[3]))
    sigma = estimate_dispersal(locations=locations, shared_times_inverted=stss_inv, shared_times_logdet=stss_logdet,
                               branching_times=btss, logpcoals=lpcs,
                               callbackF=callbackF)
    with open(output.sigma, 'w') as f:
      f.write(','.join([str(i) for i in sigma])) #save

# ----------------------- locate ancestors -----------------------

# finally, we use our processed times and dispersal rate to locate the genetic ancestor at a particular locus for a particular sample and time

ancestor_locations = processed_times.replace('.{end}','_{s}s_{t}t.locs')

rule locate_ancestors:
  input:
    stss = shared_times,
    stss_inv = processed_times.replace('{end}','stss_inv'),
    btss = processed_times.replace('{end}','btss'),
    lpcs = processed_times.replace('{end}','lpcs'),
    locations = locations,
    sigma = dispersal_rate
  output:
    ancestor_locations
  threads: 1
  resources:
    runtime=15
  run:
    # prevent numpy from using more than {threads} threads (useful for parallizing on my server)
    import os
    os.environ["OMP_NUM_THREADS"] = str(threads)
    os.environ["GOTO_NUM_THREADS"] = str(threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(threads)
    os.environ["MKL_NUM_THREADS"] = str(threads)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(threads)

    # load tools
    import numpy as np
    from tqdm import tqdm
    from spacetrees import locate_ancestors, _log_birth_density, _sds_rho_to_sigma 
    from utils import chop_shared_times

    T = wildcards.T #get time cutoff
    T = None if T=='None' else float(T) #format correctly

    n = int(wildcards.G)*int(wildcards.k) #number of samples
    
    s = wildcards.s
    if s == 'All': #an option to locate the ancestors of all samples
      samples = range(10)   
    else:
      samples = [int(s)]
    t = wildcards.t
    if t == 'All': #an option to locate at pretermined list of times 
      times = ancestor_times
    else: 
      times = [float(t)]

    # load input data
    locations = np.loadtxt(input.locations) #location of each sample
    sigma = np.loadtxt(input.sigma, delimiter=',') #mle dispersal rate and branching rate
    phi = sigma[-1] #branching rate
    sigma = _sds_rho_to_sigma(sigma[:-1]) #dispersal as covariance matrix
    
    with open(input.stss, 'r') as stss:
      with open(input.stss_inv, 'r') as stss_inv:
        with open(input.btss, 'r') as btss:
          with open(input.lpcs, 'r') as lpcs:
            with open(output[0], 'a') as f:

              i = 0
              sts_list = []
              sts_inv_list = []
              log_weights = []
              for sts, sts_inv, bts, lpc in tqdm(zip(stss, stss_inv, btss, lpcs), total=int(wildcards.m)*int(wildcards.n)):

                sts = np.fromstring(sts, dtype=float, sep=',') #shared times as vector
                sts = chop_shared_times(sts, T=T) #chop shared times to ignore history beyond T
                mat = np.zeros((n,n)) #set up emptry matrix
                mat[np.triu_indices(n, k=1)] = sts[1:] #fill in upper triangular
                mat = mat + mat.T + np.diag([sts[0]]*n) #add lower triangular and diagonal
                sts_list.append(mat) #add to list of matrices at this locus
                
                sts_inv = np.fromstring(sts_inv, dtype=float, sep=',') #inverted shared times as vector
                mat = np.zeros((n-1,n-1)) #set up emptry matrix
                mat[np.triu_indices(n-1, k=0)] = sts_inv #fill in upper triangular and diagonal
                mat = mat + mat.T - np.diag(np.diag(mat)) #add lower triangular and diagonal and subtract one diagonal
                sts_inv_list.append(mat) #add to list of matrices at this locus
        
                bts = np.fromstring(bts, dtype=float, sep=',') #coalescence times in ascending order
                lpb = _log_birth_density(bts, phi, n) #log probability of birth times
                lpc = float(lpc) #log probability of coalescence times
                log_weights.append(lpb - lpc)

                i += 1

                if i==int(wildcards.m):
        
                  ancestor_locations = locate_ancestors(samples=samples, times=times, 
                                                        shared_times_chopped=sts_list, 
                                                        shared_times_chopped_centered_inverted=sts_inv_list,
                                                        locations=locations, 
                                                        sigma=sigma, log_weights=log_weights)

                  for anc_loc in ancestor_locations:
                    f.write(','.join([str(int(anc_loc[0]))] + [str(i) for i in anc_loc[1:]]) + '\n') #save
                  
                  i = 0
                  sts_list = []
                  sts_inv_list = []
                  log_weights = []

# ---------------- dummy rule to run everything you need -----------------

rule all:
  input:
    expand(ancestor_locations, L=Ls, RBP=RBPs, LAMBDA=LAMBDAs, K=Ks, W=Ws, SIGMAcomp=SIGMAcomps, SIGMAmate=SIGMAmates, SIGMAdisp=SIGMAdisps, selfing=selfings, MAXT=MAXTs, nrep=nreps, Ne=Nes, U=Us, d=ds, k=ks, G=Gs, numiter=numiters, threshold=thresholds, treeskip=treeskips, M=Ms, T=Ts, s=['All'], t=['All'], m=ms, n=ns) 

